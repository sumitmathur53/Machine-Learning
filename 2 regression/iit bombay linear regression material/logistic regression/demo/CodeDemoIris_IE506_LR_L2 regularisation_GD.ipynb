{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eyYjJtprEncK6LSvEs9Yk37LJLtrmf1z","timestamp":1675150459323},{"file_id":"1t9BVjtuiuLvTlppuqzWTOWaGvfuKBp5h","timestamp":1675094047684},{"file_id":"1JdYIIv5htb_5vwbcrAScbU7TH87QK85M","timestamp":1675066147309},{"file_id":"1vrhoJFfzVB0C5E4A92jUVjPsRyQi2Gop","timestamp":1675064032952},{"file_id":"1Z7itiQrhFJVCMw0zxRG0hJXKFwRhWq7v","timestamp":1675020942668},{"file_id":"1c51_EzUcHt8BiGcQhHJfJ_lO5BgaLMcp","timestamp":1674794497723},{"file_id":"13jBUnq49sVoEeF0mRnMR7uOEREQ1LlbO","timestamp":1634963929543}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dE9sqluEjGoz"},"source":["#Logistic Regression in Practice"]},{"cell_type":"markdown","metadata":{"id":"ksFVSACCq3wP"},"source":["In this session, we shall apply logistic regression and look at predictions using it."]},{"cell_type":"code","metadata":{"id":"cArLwvFdqyLV"},"source":["#First, we import the required packages\n","import pandas as pd #the pandas library is useful for data processing \n","import numpy as np #numpy package will be useful for numerical computations\n","import matplotlib.pyplot as plt #the matplotlib library is useful for plotting purposes\n","\n","# The following python directive helps to plot the graph in the notebook directly\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imSI5BjGrNMW"},"source":["Now let us consider some open source data sets available in the internet."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdpI4baerHDw","executionInfo":{"status":"ok","timestamp":1675143421983,"user_tz":-330,"elapsed":611,"user":{"displayName":"Balamurugan Palaniappan","userId":"12579718159652831425"}},"outputId":"bd5b406c-fac5-4740-fbd8-a68384a25579"},"source":["from sklearn.datasets import load_iris  #importing the load_iris class\n","iris_data = load_iris() #loading the iris dataset in iris_data\n","\n","print(iris_data['DESCR']) #checking out the description of the dataset"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _iris_dataset:\n","\n","Iris plants dataset\n","--------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 150 (50 in each of three classes)\n","    :Number of Attributes: 4 numeric, predictive attributes and the class\n","    :Attribute Information:\n","        - sepal length in cm\n","        - sepal width in cm\n","        - petal length in cm\n","        - petal width in cm\n","        - class:\n","                - Iris-Setosa\n","                - Iris-Versicolour\n","                - Iris-Virginica\n","                \n","    :Summary Statistics:\n","\n","    ============== ==== ==== ======= ===== ====================\n","                    Min  Max   Mean    SD   Class Correlation\n","    ============== ==== ==== ======= ===== ====================\n","    sepal length:   4.3  7.9   5.84   0.83    0.7826\n","    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n","    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n","    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n","    ============== ==== ==== ======= ===== ====================\n","\n","    :Missing Attribute Values: None\n","    :Class Distribution: 33.3% for each of 3 classes.\n","    :Creator: R.A. Fisher\n","    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n","    :Date: July, 1988\n","\n","The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n","from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n","Machine Learning Repository, which has two wrong data points.\n","\n","This is perhaps the best known database to be found in the\n","pattern recognition literature.  Fisher's paper is a classic in the field and\n","is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n","data set contains 3 classes of 50 instances each, where each class refers to a\n","type of iris plant.  One class is linearly separable from the other 2; the\n","latter are NOT linearly separable from each other.\n","\n",".. topic:: References\n","\n","   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n","     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n","     Mathematical Statistics\" (John Wiley, NY, 1950).\n","   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n","     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n","   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n","     Structure and Classification Rule for Recognition in Partially Exposed\n","     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n","     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n","   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n","     on Information Theory, May 1972, 431-433.\n","   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n","     conceptual clustering system finds 3 classes in the data.\n","   - Many, many more ...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"siTer_VR1wiG","executionInfo":{"status":"ok","timestamp":1675143421985,"user_tz":-330,"elapsed":16,"user":{"displayName":"Balamurugan Palaniappan","userId":"12579718159652831425"}},"outputId":"e0dcd74d-f4cb-4781-bee9-649298c39c1d"},"source":["iris_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data': array([[5.1, 3.5, 1.4, 0.2],\n","        [4.9, 3. , 1.4, 0.2],\n","        [4.7, 3.2, 1.3, 0.2],\n","        [4.6, 3.1, 1.5, 0.2],\n","        [5. , 3.6, 1.4, 0.2],\n","        [5.4, 3.9, 1.7, 0.4],\n","        [4.6, 3.4, 1.4, 0.3],\n","        [5. , 3.4, 1.5, 0.2],\n","        [4.4, 2.9, 1.4, 0.2],\n","        [4.9, 3.1, 1.5, 0.1],\n","        [5.4, 3.7, 1.5, 0.2],\n","        [4.8, 3.4, 1.6, 0.2],\n","        [4.8, 3. , 1.4, 0.1],\n","        [4.3, 3. , 1.1, 0.1],\n","        [5.8, 4. , 1.2, 0.2],\n","        [5.7, 4.4, 1.5, 0.4],\n","        [5.4, 3.9, 1.3, 0.4],\n","        [5.1, 3.5, 1.4, 0.3],\n","        [5.7, 3.8, 1.7, 0.3],\n","        [5.1, 3.8, 1.5, 0.3],\n","        [5.4, 3.4, 1.7, 0.2],\n","        [5.1, 3.7, 1.5, 0.4],\n","        [4.6, 3.6, 1. , 0.2],\n","        [5.1, 3.3, 1.7, 0.5],\n","        [4.8, 3.4, 1.9, 0.2],\n","        [5. , 3. , 1.6, 0.2],\n","        [5. , 3.4, 1.6, 0.4],\n","        [5.2, 3.5, 1.5, 0.2],\n","        [5.2, 3.4, 1.4, 0.2],\n","        [4.7, 3.2, 1.6, 0.2],\n","        [4.8, 3.1, 1.6, 0.2],\n","        [5.4, 3.4, 1.5, 0.4],\n","        [5.2, 4.1, 1.5, 0.1],\n","        [5.5, 4.2, 1.4, 0.2],\n","        [4.9, 3.1, 1.5, 0.2],\n","        [5. , 3.2, 1.2, 0.2],\n","        [5.5, 3.5, 1.3, 0.2],\n","        [4.9, 3.6, 1.4, 0.1],\n","        [4.4, 3. , 1.3, 0.2],\n","        [5.1, 3.4, 1.5, 0.2],\n","        [5. , 3.5, 1.3, 0.3],\n","        [4.5, 2.3, 1.3, 0.3],\n","        [4.4, 3.2, 1.3, 0.2],\n","        [5. , 3.5, 1.6, 0.6],\n","        [5.1, 3.8, 1.9, 0.4],\n","        [4.8, 3. , 1.4, 0.3],\n","        [5.1, 3.8, 1.6, 0.2],\n","        [4.6, 3.2, 1.4, 0.2],\n","        [5.3, 3.7, 1.5, 0.2],\n","        [5. , 3.3, 1.4, 0.2],\n","        [7. , 3.2, 4.7, 1.4],\n","        [6.4, 3.2, 4.5, 1.5],\n","        [6.9, 3.1, 4.9, 1.5],\n","        [5.5, 2.3, 4. , 1.3],\n","        [6.5, 2.8, 4.6, 1.5],\n","        [5.7, 2.8, 4.5, 1.3],\n","        [6.3, 3.3, 4.7, 1.6],\n","        [4.9, 2.4, 3.3, 1. ],\n","        [6.6, 2.9, 4.6, 1.3],\n","        [5.2, 2.7, 3.9, 1.4],\n","        [5. , 2. , 3.5, 1. ],\n","        [5.9, 3. , 4.2, 1.5],\n","        [6. , 2.2, 4. , 1. ],\n","        [6.1, 2.9, 4.7, 1.4],\n","        [5.6, 2.9, 3.6, 1.3],\n","        [6.7, 3.1, 4.4, 1.4],\n","        [5.6, 3. , 4.5, 1.5],\n","        [5.8, 2.7, 4.1, 1. ],\n","        [6.2, 2.2, 4.5, 1.5],\n","        [5.6, 2.5, 3.9, 1.1],\n","        [5.9, 3.2, 4.8, 1.8],\n","        [6.1, 2.8, 4. , 1.3],\n","        [6.3, 2.5, 4.9, 1.5],\n","        [6.1, 2.8, 4.7, 1.2],\n","        [6.4, 2.9, 4.3, 1.3],\n","        [6.6, 3. , 4.4, 1.4],\n","        [6.8, 2.8, 4.8, 1.4],\n","        [6.7, 3. , 5. , 1.7],\n","        [6. , 2.9, 4.5, 1.5],\n","        [5.7, 2.6, 3.5, 1. ],\n","        [5.5, 2.4, 3.8, 1.1],\n","        [5.5, 2.4, 3.7, 1. ],\n","        [5.8, 2.7, 3.9, 1.2],\n","        [6. , 2.7, 5.1, 1.6],\n","        [5.4, 3. , 4.5, 1.5],\n","        [6. , 3.4, 4.5, 1.6],\n","        [6.7, 3.1, 4.7, 1.5],\n","        [6.3, 2.3, 4.4, 1.3],\n","        [5.6, 3. , 4.1, 1.3],\n","        [5.5, 2.5, 4. , 1.3],\n","        [5.5, 2.6, 4.4, 1.2],\n","        [6.1, 3. , 4.6, 1.4],\n","        [5.8, 2.6, 4. , 1.2],\n","        [5. , 2.3, 3.3, 1. ],\n","        [5.6, 2.7, 4.2, 1.3],\n","        [5.7, 3. , 4.2, 1.2],\n","        [5.7, 2.9, 4.2, 1.3],\n","        [6.2, 2.9, 4.3, 1.3],\n","        [5.1, 2.5, 3. , 1.1],\n","        [5.7, 2.8, 4.1, 1.3],\n","        [6.3, 3.3, 6. , 2.5],\n","        [5.8, 2.7, 5.1, 1.9],\n","        [7.1, 3. , 5.9, 2.1],\n","        [6.3, 2.9, 5.6, 1.8],\n","        [6.5, 3. , 5.8, 2.2],\n","        [7.6, 3. , 6.6, 2.1],\n","        [4.9, 2.5, 4.5, 1.7],\n","        [7.3, 2.9, 6.3, 1.8],\n","        [6.7, 2.5, 5.8, 1.8],\n","        [7.2, 3.6, 6.1, 2.5],\n","        [6.5, 3.2, 5.1, 2. ],\n","        [6.4, 2.7, 5.3, 1.9],\n","        [6.8, 3. , 5.5, 2.1],\n","        [5.7, 2.5, 5. , 2. ],\n","        [5.8, 2.8, 5.1, 2.4],\n","        [6.4, 3.2, 5.3, 2.3],\n","        [6.5, 3. , 5.5, 1.8],\n","        [7.7, 3.8, 6.7, 2.2],\n","        [7.7, 2.6, 6.9, 2.3],\n","        [6. , 2.2, 5. , 1.5],\n","        [6.9, 3.2, 5.7, 2.3],\n","        [5.6, 2.8, 4.9, 2. ],\n","        [7.7, 2.8, 6.7, 2. ],\n","        [6.3, 2.7, 4.9, 1.8],\n","        [6.7, 3.3, 5.7, 2.1],\n","        [7.2, 3.2, 6. , 1.8],\n","        [6.2, 2.8, 4.8, 1.8],\n","        [6.1, 3. , 4.9, 1.8],\n","        [6.4, 2.8, 5.6, 2.1],\n","        [7.2, 3. , 5.8, 1.6],\n","        [7.4, 2.8, 6.1, 1.9],\n","        [7.9, 3.8, 6.4, 2. ],\n","        [6.4, 2.8, 5.6, 2.2],\n","        [6.3, 2.8, 5.1, 1.5],\n","        [6.1, 2.6, 5.6, 1.4],\n","        [7.7, 3. , 6.1, 2.3],\n","        [6.3, 3.4, 5.6, 2.4],\n","        [6.4, 3.1, 5.5, 1.8],\n","        [6. , 3. , 4.8, 1.8],\n","        [6.9, 3.1, 5.4, 2.1],\n","        [6.7, 3.1, 5.6, 2.4],\n","        [6.9, 3.1, 5.1, 2.3],\n","        [5.8, 2.7, 5.1, 1.9],\n","        [6.8, 3.2, 5.9, 2.3],\n","        [6.7, 3.3, 5.7, 2.5],\n","        [6.7, 3. , 5.2, 2.3],\n","        [6.3, 2.5, 5. , 1.9],\n","        [6.5, 3. , 5.2, 2. ],\n","        [6.2, 3.4, 5.4, 2.3],\n","        [5.9, 3. , 5.1, 1.8]]),\n"," 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n"," 'frame': None,\n"," 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n"," 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n"," 'feature_names': ['sepal length (cm)',\n","  'sepal width (cm)',\n","  'petal length (cm)',\n","  'petal width (cm)'],\n"," 'filename': 'iris.csv',\n"," 'data_module': 'sklearn.datasets.data'}"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZMDdCGmewUpO","executionInfo":{"status":"ok","timestamp":1675143422456,"user_tz":-330,"elapsed":4,"user":{"displayName":"Balamurugan Palaniappan","userId":"12579718159652831425"}},"outputId":"fea80d72-3487-4556-9090-e790da855bbf"},"source":["X = iris_data['data']\n","print(X)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[5.1 3.5 1.4 0.2]\n"," [4.9 3.  1.4 0.2]\n"," [4.7 3.2 1.3 0.2]\n"," [4.6 3.1 1.5 0.2]\n"," [5.  3.6 1.4 0.2]\n"," [5.4 3.9 1.7 0.4]\n"," [4.6 3.4 1.4 0.3]\n"," [5.  3.4 1.5 0.2]\n"," [4.4 2.9 1.4 0.2]\n"," [4.9 3.1 1.5 0.1]\n"," [5.4 3.7 1.5 0.2]\n"," [4.8 3.4 1.6 0.2]\n"," [4.8 3.  1.4 0.1]\n"," [4.3 3.  1.1 0.1]\n"," [5.8 4.  1.2 0.2]\n"," [5.7 4.4 1.5 0.4]\n"," [5.4 3.9 1.3 0.4]\n"," [5.1 3.5 1.4 0.3]\n"," [5.7 3.8 1.7 0.3]\n"," [5.1 3.8 1.5 0.3]\n"," [5.4 3.4 1.7 0.2]\n"," [5.1 3.7 1.5 0.4]\n"," [4.6 3.6 1.  0.2]\n"," [5.1 3.3 1.7 0.5]\n"," [4.8 3.4 1.9 0.2]\n"," [5.  3.  1.6 0.2]\n"," [5.  3.4 1.6 0.4]\n"," [5.2 3.5 1.5 0.2]\n"," [5.2 3.4 1.4 0.2]\n"," [4.7 3.2 1.6 0.2]\n"," [4.8 3.1 1.6 0.2]\n"," [5.4 3.4 1.5 0.4]\n"," [5.2 4.1 1.5 0.1]\n"," [5.5 4.2 1.4 0.2]\n"," [4.9 3.1 1.5 0.2]\n"," [5.  3.2 1.2 0.2]\n"," [5.5 3.5 1.3 0.2]\n"," [4.9 3.6 1.4 0.1]\n"," [4.4 3.  1.3 0.2]\n"," [5.1 3.4 1.5 0.2]\n"," [5.  3.5 1.3 0.3]\n"," [4.5 2.3 1.3 0.3]\n"," [4.4 3.2 1.3 0.2]\n"," [5.  3.5 1.6 0.6]\n"," [5.1 3.8 1.9 0.4]\n"," [4.8 3.  1.4 0.3]\n"," [5.1 3.8 1.6 0.2]\n"," [4.6 3.2 1.4 0.2]\n"," [5.3 3.7 1.5 0.2]\n"," [5.  3.3 1.4 0.2]\n"," [7.  3.2 4.7 1.4]\n"," [6.4 3.2 4.5 1.5]\n"," [6.9 3.1 4.9 1.5]\n"," [5.5 2.3 4.  1.3]\n"," [6.5 2.8 4.6 1.5]\n"," [5.7 2.8 4.5 1.3]\n"," [6.3 3.3 4.7 1.6]\n"," [4.9 2.4 3.3 1. ]\n"," [6.6 2.9 4.6 1.3]\n"," [5.2 2.7 3.9 1.4]\n"," [5.  2.  3.5 1. ]\n"," [5.9 3.  4.2 1.5]\n"," [6.  2.2 4.  1. ]\n"," [6.1 2.9 4.7 1.4]\n"," [5.6 2.9 3.6 1.3]\n"," [6.7 3.1 4.4 1.4]\n"," [5.6 3.  4.5 1.5]\n"," [5.8 2.7 4.1 1. ]\n"," [6.2 2.2 4.5 1.5]\n"," [5.6 2.5 3.9 1.1]\n"," [5.9 3.2 4.8 1.8]\n"," [6.1 2.8 4.  1.3]\n"," [6.3 2.5 4.9 1.5]\n"," [6.1 2.8 4.7 1.2]\n"," [6.4 2.9 4.3 1.3]\n"," [6.6 3.  4.4 1.4]\n"," [6.8 2.8 4.8 1.4]\n"," [6.7 3.  5.  1.7]\n"," [6.  2.9 4.5 1.5]\n"," [5.7 2.6 3.5 1. ]\n"," [5.5 2.4 3.8 1.1]\n"," [5.5 2.4 3.7 1. ]\n"," [5.8 2.7 3.9 1.2]\n"," [6.  2.7 5.1 1.6]\n"," [5.4 3.  4.5 1.5]\n"," [6.  3.4 4.5 1.6]\n"," [6.7 3.1 4.7 1.5]\n"," [6.3 2.3 4.4 1.3]\n"," [5.6 3.  4.1 1.3]\n"," [5.5 2.5 4.  1.3]\n"," [5.5 2.6 4.4 1.2]\n"," [6.1 3.  4.6 1.4]\n"," [5.8 2.6 4.  1.2]\n"," [5.  2.3 3.3 1. ]\n"," [5.6 2.7 4.2 1.3]\n"," [5.7 3.  4.2 1.2]\n"," [5.7 2.9 4.2 1.3]\n"," [6.2 2.9 4.3 1.3]\n"," [5.1 2.5 3.  1.1]\n"," [5.7 2.8 4.1 1.3]\n"," [6.3 3.3 6.  2.5]\n"," [5.8 2.7 5.1 1.9]\n"," [7.1 3.  5.9 2.1]\n"," [6.3 2.9 5.6 1.8]\n"," [6.5 3.  5.8 2.2]\n"," [7.6 3.  6.6 2.1]\n"," [4.9 2.5 4.5 1.7]\n"," [7.3 2.9 6.3 1.8]\n"," [6.7 2.5 5.8 1.8]\n"," [7.2 3.6 6.1 2.5]\n"," [6.5 3.2 5.1 2. ]\n"," [6.4 2.7 5.3 1.9]\n"," [6.8 3.  5.5 2.1]\n"," [5.7 2.5 5.  2. ]\n"," [5.8 2.8 5.1 2.4]\n"," [6.4 3.2 5.3 2.3]\n"," [6.5 3.  5.5 1.8]\n"," [7.7 3.8 6.7 2.2]\n"," [7.7 2.6 6.9 2.3]\n"," [6.  2.2 5.  1.5]\n"," [6.9 3.2 5.7 2.3]\n"," [5.6 2.8 4.9 2. ]\n"," [7.7 2.8 6.7 2. ]\n"," [6.3 2.7 4.9 1.8]\n"," [6.7 3.3 5.7 2.1]\n"," [7.2 3.2 6.  1.8]\n"," [6.2 2.8 4.8 1.8]\n"," [6.1 3.  4.9 1.8]\n"," [6.4 2.8 5.6 2.1]\n"," [7.2 3.  5.8 1.6]\n"," [7.4 2.8 6.1 1.9]\n"," [7.9 3.8 6.4 2. ]\n"," [6.4 2.8 5.6 2.2]\n"," [6.3 2.8 5.1 1.5]\n"," [6.1 2.6 5.6 1.4]\n"," [7.7 3.  6.1 2.3]\n"," [6.3 3.4 5.6 2.4]\n"," [6.4 3.1 5.5 1.8]\n"," [6.  3.  4.8 1.8]\n"," [6.9 3.1 5.4 2.1]\n"," [6.7 3.1 5.6 2.4]\n"," [6.9 3.1 5.1 2.3]\n"," [5.8 2.7 5.1 1.9]\n"," [6.8 3.2 5.9 2.3]\n"," [6.7 3.3 5.7 2.5]\n"," [6.7 3.  5.2 2.3]\n"," [6.3 2.5 5.  1.9]\n"," [6.5 3.  5.2 2. ]\n"," [6.2 3.4 5.4 2.3]\n"," [5.9 3.  5.1 1.8]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"gxwtNOdw-Xfb"},"source":["From the description, it is clear that *iris* dataset consists of essentially 3 classes: *Iris-Setosa*, *Iris-Versicolour*, *Iris-Virginica*. As logistic regression has been introduced as a binary classifier, we can alter the data into a binary classification problem based on finding whether a flower belongs to *Iris-Virginica* or not. In order to do that, we change the labels $\\{0,1\\}$ to $0$ and 2 to 1."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zyhWD_Pr3_dR","executionInfo":{"status":"ok","timestamp":1675143424385,"user_tz":-330,"elapsed":11,"user":{"displayName":"Balamurugan Palaniappan","userId":"12579718159652831425"}},"outputId":"217a25e6-73da-4e4a-9feb-dd87f999fbc4"},"source":["y = np.where(iris_data['target'] == 2, 1, 0) #shorthand notation to change all labels other than 2 as 0 and 2 as 1\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1]\n"]}]},{"cell_type":"markdown","source":["$\\Large{\\text{Logistic Regression with L2 regularisation} \\ \\text{ }}:$"],"metadata":{"id":"B1QtSSUHNIJw"}},{"cell_type":"markdown","metadata":{"id":"QMalZbFjy8w0"},"source":["Assume that we have a random variable $X$ whose realization is $x$ a $d$-dimensional data:  \n","$\n","x=\n","\\begin{bmatrix}\n","x_1 \\\\\n","x_2 \\\\\n","\\vdots\\\\\n","x_d\n","\\end{bmatrix}$.\n","Now we want to model the response variable $Y$ using logistic regression. \n","\n","Recall that we use $E[Y|X=x] = p(x)$. \n","\n","Also recall that we write $p(x)$ equivalently as:\n","$\n","p(x)=p(x;\\beta_0,\\beta_1,\\ldots,\\beta_d) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_d x_d)}}.\n","$\n","\n","If we denote $p(x)$ simply as $p$ and if we have the notations $\\mathbf{x}=\\begin{bmatrix}\n","x \\\\ 1\n","\\end{bmatrix} = \\begin{bmatrix}\n","x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d \\\\ 1\n","\\end{bmatrix}, \\beta=\\begin{bmatrix}\n","\\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_d \\\\ \\beta_0\n","\\end{bmatrix}$\n","then we can write:\n","\n","$\n","\\begin{align}\n","p = \\frac{1}{1+e^{-\\beta^\\top \\mathbf{x}}}.\n","\\end{align}\n","$\n","\n","We also derived that\n","\n","$\n","\\begin{align}\n","\\beta^\\top \\mathbf{x} &= \\ln \\frac{p}{1-p} \\\\\n","\\implies \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_d x_d &= \\ln \\frac{p}{1-p}.\n","\\end{align}\n","$\n","\n","Thus, even if we did not have a straightforward dependence of $Y$ on an observation $x$ of $X$ as a linear relation, we see that the linear relation $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_d x_d $ is related to the probability $p$ using:\n","\n","$\n","\\ln\\frac{p}{1-p}=\\beta^\\top \\mathbf{x}.\n","$\n","\n","Note that the ratio $\\frac{p}{1-p}$ is called $\\textbf{odds}$ that the event $Y=1$ occurs, and hence $\\ln \\frac{p}{1-p}$ denotes the $\\textbf{log odds}$. \n","\n","More popularly, the log odds $\\ln \\frac{p}{1-p}$ is called the $\\textbf{logit}$ function. \n"]},{"cell_type":"markdown","metadata":{"id":"4h5j_iGwAxgt"},"source":["$\\Large{\\text{Likelihood function}}$ \n","\n","We defined a quantity useful in the estimation of the parameters $\\beta$ used to model $p$.\n","\n","\n","Given an observation $X=x=\\begin{bmatrix}\n","x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_d\n","\\end{bmatrix}$ \n","we define the $\\textbf{likelihood function}$ as: \n","\n","$\n","L(y;p) = p^y(1-p)^{(1-y)}\n","$\n","\n","where recall that $p=p(x)=p(x;\\beta)=\\frac{1}{1+e^{-\\beta^\\top {\\mathbf{x}}}}$. Note that likelihood function is simply an equivalent way to represent $P[Y=y]$, when $Y$ is assumed to be Bernoulli random variable. \n","\n","Then observe that the natural goal is to maximize the likelihood function with respect to parameters $\\beta$. \n","\n","Now given a data set $D$ containing $n$ observations of the form $\\{({x}^1,y^1), ({x}^2,y^2), \\ldots, ({x}^n,y^n)\\}$, and assuming that the pairs $({x}^i,y^i)$ are independent observations, then it is possible to extend the likelihood function as: \n","\n","$\n","\\begin{align}\n","L(y^1,\\ldots,y^n;p^1,\\ldots,p^n) = \\Pi_{i=1}^{n} {(p^i)}^{y^i}(1-p^i)^{(1-y^i)}.\n","\\end{align}\n","$\n","\n","We can now write the $\\textbf{log likelihood}$ function as:\n","\n","$\n","\\begin{align}\n","\\ln L(y^1,\\ldots,y^n;p^1,\\ldots,p^n) = \\sum_{i=1}^{n} y^i \\ln {(p^i)} + (1-y^i) \\ln (1-p^i).\n","\\end{align}\n","$\n","\n","Since natural log function is monotonic, maximizing the likelihood function is equivalent to maximizing the log likelihood function. \n","\n","Hence the concerned optimization problem is: \n","\n","$\n","\\max_{\\beta} \\ln L(y^1,\\ldots,y^n;p^1;\\ldots,p^n)= \\sum_{i=1}^{n} y^i \\ln {(p^i)} + (1-y^i) \\ln (1-p^i).\n","$\n","\n","Note that $p^i = p(x^i) = p(x^i; \\beta) = \\frac{1}{1+e^{-{\\beta^\\top\\mathbf{x}}}}, \\forall i = 1,\\ldots,n$.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7MBRXqhZBAn2"},"source":["$\\Large{\\text{Solving the regularized likelihood maximization problem}}:$\n","\n","To solve \n","\n","$\n","\\max_{\\beta} f(\\beta) = \\ln L(y^1,\\ldots,y^n;p^1;\\ldots,p^n) - \\frac{\\lambda}{2} \\|\\beta\\|_2^2 = \\sum_{i=1}^{n} y^i \\ln {(p^i)} + (1-y^i) \\ln (1-p^i) - \\frac{\\lambda}{2} \\|\\beta\\|_2^2 - \\frac{\\lambda}{2} \\|\\beta\\|_2^2.\n","$\n","\n","we can find the gradient of the objective function with respect to $\\beta$ as:\n","\n","$\n","\\begin{align}\n","\\nabla_\\beta {\\ln L} =  \\begin{bmatrix}\n","\\frac{\\partial{ \\ln L }} {\\partial \\beta_1} \\\\\n","\\frac{\\partial{ \\ln L }} {\\partial \\beta_2} \\\\\n","\\vdots \\\\\n","\\frac{\\partial{ \\ln L }} {\\partial \\beta_d} \\\\\n","\\frac{\\partial{ \\ln L }} {\\partial \\beta_0} \n","\\end{bmatrix}\n","&= \\sum_{i=1}^{n} \\frac{y^i}{p^i} \\nabla_\\beta  p^i + \\frac{(1-y^i)}{(1-p^i)} \\nabla_\\beta (1-p^i) - \\lambda \\beta \\\\ \n","&= \\sum_{i=1}^{n} \\frac{y^i}{p^i} \\nabla_\\beta  p^i - \\frac{(1-y^i)}{(1-p^i)} \\nabla_\\beta p^i \n","\\end{align}\n","$\n","\n","where $\\nabla_\\beta  p^i = \\nabla_\\beta  \\left ( \\frac{1}{1+e^{-\\beta^\\top {\\mathbf{x}}^i}}\\right )$.\n","\n","Note that $\\nabla_\\beta p^i$ can be computed as:\n","$\n","\\begin{align}\n","\\nabla_\\beta p^i = \\nabla_\\beta \\frac{1}{1+e^{-\\beta^\\top {\\mathbf{x}}^i}} = \n","\\frac{e^{-\\beta^\\top {\\mathbf{x}}^i}} {(1+e^{-\\beta^\\top {\\mathbf{x}}^i})^2}\n","{\\mathbf{x}}^i=\n","\\left (\\frac{1}{1+e^{-\\beta^\\top {\\mathbf{x}}^i}}\\right ) \\left (\\frac{e^{-\\beta^\\top {\\mathbf{x}}^i}}{1+e^{-\\beta^\\top {\\mathbf{x}}^i}}\\right ) {\\mathbf{x}}^i =   \n","p^i(1-p^i) {\\mathbf{x}}^i\n","\\end{align}\n","$\n","\n","Thus we have \n","\n","$\n","\\begin{align}\n","\\nabla_\\beta {\\ln L} &= \\sum_{i=1}^{n} \\frac{y^i}{p^i} \\nabla_\\beta  p^i - \\frac{(1-y^i)}{(1-p^i)} \\nabla_\\beta p^i  \\\\ \n","&=  \\sum_{i=1}^{n} \\frac{y^i}{p^i} p^i(1-p^i){\\mathbf{x}}^i - \\frac{(1-y^i)}{(1-p^i)} p^i(1-p^i){\\mathbf{x}}^i \\\\\n","&= \\sum_{i=1}^{n} \\left(y^i (1-p^i) - (1-y^i) p^i\\right ){\\mathbf{x}}^i\\\\\n","&= \\sum_{i=1}^{n} \\left(y^i - y^ i p^i - p^i + y^i p^i\\right ){\\mathbf{x}}^i \\\\\n","&= \\sum_{i=1}^{n} \\left(y^i - p^i \\right ){\\mathbf{x}}^i\n","\\end{align}\n","$\n","\n","Hence the overall gradient of the regularized objective function is:\n","\n","$\n","\\nabla_{\\beta} f(\\beta) = \\sum_{i=1}^{n} \\left(y^i - p^i \\right ) - \\lambda \\beta\n","$\n","\n","We generally adopt an iterative procedure as follows to find the optimal $\\beta$. \n","\n","$\\large{\\text{Gradient ascent for solving the likelihood maximization problem}}:$\n","\n","$\n","\\begin{align}\n","&\\textbf{Step 0:}  \\text{Input data set $D$, tolerances $\\epsilon_1, \\epsilon_2$.} \\\\\n","&\\textbf{Step 1:}  \\text{Start with arbitrary $\\beta_0$.} \\\\\n","&\\textbf{Step 2:}  \\text{For $k=0,1,2,\\ldots$} \\\\\n","&\\quad \\quad \\textbf{Step 2.1:} \\text{Compute gradient $\\nabla_\\beta f(\\beta_k)$}. \\\\\n","&\\quad \\quad \\textbf{Step 2.2:}  \\text{Compute step length $\\eta$ using line search procedure} \\\\\n","&\\quad \\quad \\textbf{Step 2.3:}  \\beta_{k+1} = \\beta_{k} + \\eta_k \\nabla_\\beta f(\\beta_k) \\\\\n","&\\quad \\quad \\textbf{Step 2.4:}  \\text{if $\\|\\nabla_{\\beta} f(\\beta_{k+1})\\|_2 \\leq \\epsilon_1$ break from loop} \\\\\n","&\\quad \\quad \\textbf{Step 2.5:}  \\text{if relative change in function value is $\\leq \\epsilon_2$ break from loop} \\\\\n","&\\textbf{Step 3:}  \\text{ Output $\\beta_{k+1}$}\n","\\end{align}\n","$\n","\n","\n","Note $\\eta_k$ denotes the learning rate. \n"]},{"cell_type":"markdown","metadata":{"id":"QtspvI6zZOo5"},"source":["$\\large{\\text{Module for objective value computation}}:$"]},{"cell_type":"code","metadata":{"id":"uI1OOOjrZOo6"},"source":["import numpy as np\n","from numpy.linalg import norm\n","#computing log likelihood function\n","def log_likelihood(beta,lamda, X, y):\n","  log_likelihood = 0.0\n","  num_samples = X.shape[0]\n","  for i in range(num_samples):\n","    x_i = X[i] #access i-th feature row\n","    y_i = float(y[i]) #access i-th label\n","    p_i = 1.0/(1.0+np.exp(-np.dot(beta, x_i))) #probability with the current beta\n","    log_likelihood += y_i * np.log(p_i) + (1.0-y_i)*np.log(1-p_i)  #adding the penalty term\n","  return log_likelihood-(lamda/2)*np.dot(beta,beta)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\large{\\text{Module for gradient computation}}:$"],"metadata":{"id":"bnk0HeaeUnR9"}},{"cell_type":"code","metadata":{"id":"dn-ElhVeZOo6"},"source":["#computing gradient of objective function with respect to beta\n","\n","def compute_gradient(beta,lamda,X,y):\n","  gradient = np.zeros(X.shape[1])\n","  num_samples = X.shape[0]\n","  for i in range(num_samples):\n","    x_i = X[i] #access i-th feature row\n","    y_i = float(y[i]) #access i-th label\n","    p_i = 1.0/(1.0+np.exp(-np.dot(beta, x_i))) #probability with the current beta\n","    gradient = np.add(gradient,(y_i -p_i)*x_i) \n","  return np.add(gradient,-lamda*beta) #adding penalty term in gradient"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675143484348,"user_tz":-330,"elapsed":429,"user":{"displayName":"Balamurugan Palaniappan","userId":"12579718159652831425"}},"outputId":"4242b0d7-f8b7-4332-a48f-1684f6414a6d","id":"7d6U3g75ZOo7"},"source":["#cross-check compute_gradient and log_likelihood functions\n","beta = np.zeros(X.shape[1])\n","print(X[0])\n","print(beta)\n","np.dot(X[0],beta)\n","print('log likelihood:',log_likelihood(beta,1e-3, np.array([X[0]]), np.array([y[0]])))\n","print('gradient:',compute_gradient(beta,1e-3,np.array([X[0]]), np.array([y[0]])))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[5.1 3.5 1.4 0.2]\n","[0. 0. 0. 0.]\n","log likelihood: -0.6931471805599453\n","gradient: [-2.55 -1.75 -0.7  -0.1 ]\n"]}]},{"cell_type":"markdown","source":["$\\large{\\text{Module for Training}}:$"],"metadata":{"id":"Vte3Uat_Us3T"}},{"cell_type":"code","source":["def solve_l2_regularized_logistic_regression(X,y, lamda, max_iter = 500000, eps_1=0.01, eps_2= 1e-9, verbose=0, plot_obj=False):\n","  #max_iter denotes maximum number of iterations\n","  #eps_1 is tolerance for gradient norm\n","  #eps_2 is tolerance for relative function value difference\n","\n","  #gradient ascent for likelihood maximization \n","  beta = np.zeros(X.shape[1])\n","\n","  f_val_list = []\n","  if verbose>3:\n","    f_val = log_likelihood(beta,lamda, X, y)\n","   #store the objective function values for plotting purposes\n","    f_val_list.append(f_val)\n","\n","  if verbose>3:\n","    print('Initial values: beta:', beta, ' log likeihood:', f_val)\n","  #the loop \n","  for k in range(max_iter):\n","    \n","    #compute gradient of objective function with respect to beta\n","    grad_beta = compute_gradient(beta,lamda, X, y)\n","      \n","    #lr = linesearch(beta_0, beta_1, grad_beta_0, grad_beta_1, float(f_val_list[-1]))\n","    lr = 0.01 #we use a constant step length (or) learning rate\n","    #update beta \n","    beta = np.add(beta, lr*grad_beta)\n","    \n","    #print('k: ', k, ' grad beta: ', grad_beta,  'beta:',beta)\n","    grad_norm = np.linalg.norm(grad_beta)\n","    if verbose>3:\n","      f_val = log_likelihood(beta,lamda, X, y)\n","      f_val_list.append(f_val)\n","      rel_change_in_fval = np.abs((f_val - f_val_list[-2])/f_val_list[-2])\n","\n","    if verbose>3:\n","      if k%10000+1 == 1:\n","        print('k: ', k,  ' beta:', beta, ' grad norm:', grad_norm, 'log likelihood:', f_val)\n","    if grad_norm <= eps_1:# or rel_change_in_fval <= eps_2:\n","      break\n","  if verbose>=0:\n","    print('Final: k: ', k,  ' beta:', beta, ' grad norm:', grad_norm, 'log likelihood:', log_likelihood(beta,lamda, X, y))\n","\n","  if plot_obj == True:\n","    #plot the function values during optimization \n","    plt.plot( f_val_list, '-r', label='Log Likelihood values')\n","    plt.title(\"Log likelihood function values vs iterations\")\n","    plt.xlabel(\"Iteration\")\n","    plt.ylabel(\"Log Likelihood\")\n","    plt.legend(loc='lower right')\n","    #plt.grid()\n","    plt.show()\n","  return beta"],"metadata":{"id":"PcoOqbE7RR2h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\large{\\text{Module for accuracy computation}}:$"],"metadata":{"id":"kFBtWprqUzL7"}},{"cell_type":"code","source":["def compute_accuracy(beta,X,y):\n","  num_samples = X.shape[0]\n","  num_correct_predicted = 0\n","  for i in range(num_samples):\n","    x_i = X[i] #access i-th feature row\n","    y_i = float(y[i]) #access i-th label\n","    p_i = 1.0/(1.0+np.exp(-np.dot(beta, x_i))) #probability with the current beta\n","    if p_i >= 0.5:\n","      y_i_pred = 1\n","    else:\n","      y_i_pred = 0\n","    if y[i] == y_i_pred:\n","      num_correct_predicted+=1  \n","  accuracy = num_correct_predicted/num_samples\n","  print('num samples:', num_samples, 'num correct predictions: ', num_correct_predicted, 'accuracy:', accuracy)\n","  return accuracy\n"],"metadata":{"id":"oyClO1JAOF1L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$\\large{\\text{k-Fold Cross-Validation}}:$"],"metadata":{"id":"GzYcpuiOU3xt"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","seed = 2000\n","\n","X_train_initial, X_test, y_train_initial, y_test = train_test_split(X, y, test_size = 0.25, random_state = seed)\n","X_train_initial = np.hstack((X_train_initial,np.ones((X_train_initial.shape[0],1))))\n","X_test = np.hstack((X_test,np.ones((X_test.shape[0],1))))\n","\n","print('X_train_initial shape:', X_train_initial.shape)\n","print('y_train_initial shape:', y_train_initial.shape)\n","print('y_train_initial:', y_train_initial)\n","\n","print('X_test shape:', X_test.shape)\n","print('y_test shape:', y_test.shape)\n","print('y_test:', y_test)\n","\n","seeds = [100, 200, 300]\n","\n","lambdas = [0.001, 0.01, 0.1]\n","\n","train_acc_seeds = [] \n","val_acc_seeds = [] \n","\n","for seed in seeds: \n","  X_train, X_val, y_train, y_val = train_test_split(X_train_initial, y_train_initial, test_size = 0.2, random_state = seed)\n","  print('#################################################')\n","  print('K fold seed:', seed)\n","  print('X_train shape:', X_train.shape)\n","  print('y_train shape:', y_train.shape)\n","  #print('y_train:', y_train)\n","  \n","  print('X_test shape:', X_test.shape)\n","  print('y_test shape:', y_test.shape)\n","  #print('y_test:', y_test)\n","  \n","  \n","  \n","  train_acc_lambdas = []\n","  val_acc_lambdas = []\n","  \n","  for lambda_ in lambdas:\n","    print('*******************************************')\n","    print('lambda:',lambda_)\n","    beta = solve_l2_regularized_logistic_regression(X_train,y_train, lambda_)\n","    print('beta:',beta)\n","    print('***************')\n","\n","    train_acc = compute_accuracy(beta,X_train,y_train)\n","    train_acc_lambdas.append(train_acc)\n","\n","    val_acc = compute_accuracy(beta,X_val,y_val)\n","    val_acc_lambdas.append(val_acc)\n","\n","  train_acc_seeds.append(np.array(train_acc_lambdas))\n","\n","  val_acc_seeds.append(np.array(val_acc_lambdas))\n","\n","print('***************** K fold cross validation complete ! **************')\n","val_acc_seeds = np.array(val_acc_seeds).squeeze()\n","print(val_acc_seeds)\n","\n","mean_val_acc_lambdas = np.mean(val_acc_seeds,axis=0)\n","\n","train_acc_seeds = np.array(train_acc_seeds).squeeze()\n","print(train_acc_seeds)\n","\n","mean_train_acc_lambdas = np.mean(train_acc_seeds,axis=0)\n","\n","\n","print('k-fold val acc values:', mean_val_acc_lambdas.squeeze())\n","print('k-fold train acc values:', mean_train_acc_lambdas.squeeze())\n","\n","best_lambda_idx = np.argmax(mean_val_acc_lambdas)\n","best_lambda = lambdas[best_lambda_idx]\n","\n","print('*************************************')\n","print('best lambda:', best_lambda)\n","print('*************************************')\n","\n","print('Solving with full train data and best lambda:')\n","beta = solve_l2_regularized_logistic_regression(X_train_initial,y_train_initial, best_lambda)\n","print('lambda:',best_lambda)\n","print('Final beta:',beta)\n","print('**********************************')\n","\n","train_acc = compute_accuracy(beta,X_train_initial,y_train_initial)\n","\n","test_acc = compute_accuracy(beta,X_test,y_test)\n","\n","print('*************************************')\n","print('Printing train and test accuracies after training on full data and best lambda:')\n","print('Train acc :', train_acc)\n","print('Test acc:', test_acc)\n","print('************************************')\n"],"metadata":{"id":"T47TKZa7ODks","executionInfo":{"status":"ok","timestamp":1675149807423,"user_tz":-330,"elapsed":1026755,"user":{"displayName":"Balamurugan Palaniappan","userId":"12579718159652831425"}},"outputId":"73afdd11-389c-4668-b991-45bbce2dde00","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train_initial shape: (112, 5)\n","y_train_initial shape: (112,)\n","y_train_initial: [0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 1\n"," 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1\n"," 0 0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0\n"," 0]\n","X_test shape: (38, 5)\n","y_test shape: (38,)\n","y_test: [0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0\n"," 1]\n","#################################################\n","K fold seed: 100\n","X_train shape: (89, 5)\n","y_train shape: (89,)\n","X_test shape: (38, 5)\n","y_test shape: (38,)\n","*******************************************\n","lambda: 0.001\n","Final: k:  50416  beta: [-13.79412577  -5.5638296   18.70950498  14.91305099 -14.27554457]  grad norm: 0.009999883550355443 log likelihood: -1.4508850850234418\n","beta: [-13.79412577  -5.5638296   18.70950498  14.91305099 -14.27554457]\n","***************\n","num samples: 89 num correct predictions:  89 accuracy: 1.0\n","num samples: 23 num correct predictions:  23 accuracy: 1.0\n","*******************************************\n","lambda: 0.01\n","Final: k:  10927  beta: [-6.49335687 -3.67188573  8.79734213  8.85868706 -6.50431055]  grad norm: 0.009999949523061739 log likelihood: -3.4416204836796727\n","beta: [-6.49335687 -3.67188573  8.79734213  8.85868706 -6.50431055]\n","***************\n","num samples: 89 num correct predictions:  89 accuracy: 1.0\n","num samples: 23 num correct predictions:  23 accuracy: 1.0\n","*******************************************\n","lambda: 0.1\n","Final: k:  499999  beta: [-3.93747593 -2.70396635  5.41300911  5.26556923 -2.75465365]  grad norm: 0.341155507807074 log likelihood: -8.647074377164714\n","beta: [-3.93747593 -2.70396635  5.41300911  5.26556923 -2.75465365]\n","***************\n","num samples: 89 num correct predictions:  88 accuracy: 0.9887640449438202\n","num samples: 23 num correct predictions:  23 accuracy: 1.0\n","#################################################\n","K fold seed: 200\n","X_train shape: (89, 5)\n","y_train shape: (89,)\n","X_test shape: (38, 5)\n","y_test shape: (38,)\n","*******************************************\n","lambda: 0.001\n","Final: k:  48283  beta: [-12.97171742  -5.57354283  18.17409884  13.51891524 -14.65001118]  grad norm: 0.009999986993320442 log likelihood: -1.403382428803685\n","beta: [-12.97171742  -5.57354283  18.17409884  13.51891524 -14.65001118]\n","***************\n","num samples: 89 num correct predictions:  89 accuracy: 1.0\n","num samples: 23 num correct predictions:  23 accuracy: 1.0\n","*******************************************\n","lambda: 0.01\n","Final: k:  11155  beta: [-6.07945183 -3.93633265  8.8441162   7.86141828 -7.1897819 ]  grad norm: 0.00999951600220546 log likelihood: -3.287483268638741\n","beta: [-6.07945183 -3.93633265  8.8441162   7.86141828 -7.1897819 ]\n","***************\n","num samples: 89 num correct predictions:  89 accuracy: 1.0\n","num samples: 23 num correct predictions:  23 accuracy: 1.0\n","*******************************************\n","lambda: 0.1\n","Final: k:  499999  beta: [-4.89147484 -3.63556796  6.99795233  6.05558883 -3.41421174]  grad norm: 0.8400809945699881 log likelihood: -9.808171022923286\n","beta: [-4.89147484 -3.63556796  6.99795233  6.05558883 -3.41421174]\n","***************\n","num samples: 89 num correct predictions:  88 accuracy: 0.9887640449438202\n","num samples: 23 num correct predictions:  23 accuracy: 1.0\n","#################################################\n","K fold seed: 300\n","X_train shape: (89, 5)\n","y_train shape: (89,)\n","X_test shape: (38, 5)\n","y_test shape: (38,)\n","*******************************************\n","lambda: 0.001\n","Final: k:  31399  beta: [-12.19332605  -2.51490997  12.99214204  18.06571939  -9.84858972]  grad norm: 0.009999499966866745 log likelihood: -0.6891662016799056\n","beta: [-12.19332605  -2.51490997  12.99214204  18.06571939  -9.84858972]\n","***************\n","num samples: 89 num correct predictions:  89 accuracy: 1.0\n","num samples: 23 num correct predictions:  22 accuracy: 0.9565217391304348\n","*******************************************\n","lambda: 0.01\n","Final: k:  10712  beta: [-7.07750571 -2.33253469  8.1307476  10.47343167 -6.09313055]  grad norm: 0.00999859008065272 log likelihood: -2.5725762469164564\n","beta: [-7.07750571 -2.33253469  8.1307476  10.47343167 -6.09313055]\n","***************\n","num samples: 89 num correct predictions:  89 accuracy: 1.0\n","num samples: 23 num correct predictions:  22 accuracy: 0.9565217391304348\n","*******************************************\n","lambda: 0.1\n","Final: k:  499999  beta: [-3.92503156 -2.26168101  7.43727747  6.73523649 -2.94173215]  grad norm: 238.44429480779579 log likelihood: -234.2946605000487\n","beta: [-3.92503156 -2.26168101  7.43727747  6.73523649 -2.94173215]\n","***************\n","num samples: 89 num correct predictions:  58 accuracy: 0.651685393258427\n","num samples: 23 num correct predictions:  17 accuracy: 0.7391304347826086\n","***************** K fold cross validation complete ! **************\n","[[1.         1.         1.        ]\n"," [1.         1.         1.        ]\n"," [0.95652174 0.95652174 0.73913043]]\n","[[1.         1.         0.98876404]\n"," [1.         1.         0.98876404]\n"," [1.         1.         0.65168539]]\n","k-fold val acc values: [0.98550725 0.98550725 0.91304348]\n","k-fold train acc values: [1.         1.         0.87640449]\n","*************************************\n","best lambda: 0.001\n","*************************************\n","Solving with full train data and best lambda:\n","Final: k:  47390  beta: [-14.11065159  -5.74212119  19.09495235  15.30543032 -14.34729486]  grad norm: 0.009999845119496269 log likelihood: -1.4592340545286455\n","lambda: 0.001\n","Final beta: [-14.11065159  -5.74212119  19.09495235  15.30543032 -14.34729486]\n","**********************************\n","num samples: 112 num correct predictions:  112 accuracy: 1.0\n","num samples: 38 num correct predictions:  35 accuracy: 0.9210526315789473\n","*************************************\n","Printing train and test accuracies after training on full data and best lambda:\n","Train acc : 1.0\n","Test acc: 0.9210526315789473\n","************************************\n"]}]},{"cell_type":"code","source":["# plt.figure(figsize =(12,7))\n","\n","# plt.plot(lambdas,mean_train_acc_lambdas,label = \"Training accuracy value\")\n","# plt.plot(lambdas, mean_val_acc_lambdas,label = \"Validation accuracy value\")\n","# plt.xlabel('Lambda')\n","# plt.ylabel('Acc. score ')\n","# plt.title(\"Avg. acc. score vs Lambda\")\n","# plt.xscale('log')\n","# plt.grid()\n","# plt.legend()\n","# plt.xticks( [1e-3, 1e-2])\n","# plt.show()\n"],"metadata":{"id":"oI4PweGIqa3i","executionInfo":{"status":"ok","timestamp":1675154032438,"user_tz":-330,"elapsed":392,"user":{"displayName":"Balamurugan Palaniappan","userId":"12579718159652831425"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hRClhRyPlH0W"},"execution_count":null,"outputs":[]}]}